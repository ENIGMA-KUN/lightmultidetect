# Deployment Configuration Files

## docker-compose.yml
version: '3.8'

services:
  # Frontend container
  frontend:
    build: 
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    depends_on:
      - backend
    networks:
      - lightmultidetect-network
    environment:
      - REACT_APP_API_URL=http://localhost:8000/api
    restart: unless-stopped

  # Backend API container
  backend:
    build: 
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    depends_on:
      - redis
    volumes:
      - ./data:/app/data
      - ./ml/models/weights:/app/ml/models/weights
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    restart: unless-stopped
    networks:
      - lightmultidetect-network

  # Worker container for processing tasks
  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.worker
    command: celery -A backend.tasks.detection_tasks worker --loglevel=info
    volumes:
      - ./data:/app/data
      - ./ml/models/weights:/app/ml/models/weights
    depends_on:
      - redis
      - backend
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - C_FORCE_ROOT=true
    restart: unless-stopped
    networks:
      - lightmultidetect-network

  # Redis for task queue and caching
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    networks:
      - lightmultidetect-network

networks:
  lightmultidetect-network:
    driver: bridge

volumes:
  redis-data:


## frontend/Dockerfile
# Stage 1: Build React application
FROM node:18-alpine as build

WORKDIR /app

# Copy package files and install dependencies
COPY package.json package-lock.json ./
RUN npm ci

# Copy source code
COPY . .

# Build the application
RUN npm run build

# Stage 2: Serve with Nginx
FROM nginx:alpine

# Copy build from previous stage
COPY --from=build /app/build /usr/share/nginx/html

# Copy custom Nginx config
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Expose port
EXPOSE 80

# Start Nginx
CMD ["nginx", "-g", "daemon off;"]


## frontend/nginx.conf
server {
    listen 80;
    server_name localhost;

    # Compression settings
    gzip on;
    gzip_disable "msie6";
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_buffers 16 8k;
    gzip_http_version 1.1;
    gzip_min_length 256;
    gzip_types
        application/atom+xml
        application/geo+json
        application/javascript
        application/x-javascript
        application/json
        application/ld+json
        application/manifest+json
        application/rdf+xml
        application/rss+xml
        application/xhtml+xml
        application/xml
        font/eot
        font/otf
        font/ttf
        image/svg+xml
        text/css
        text/javascript
        text/plain
        text/xml;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN";
    add_header X-XSS-Protection "1; mode=block";
    add_header X-Content-Type-Options "nosniff";
    add_header Referrer-Policy "no-referrer-when-downgrade";

    location / {
        root /usr/share/nginx/html;
        index index.html index.htm;
        try_files $uri $uri/ /index.html;
    }

    # Cache static assets
    location ~* \.(jpg|jpeg|png|gif|ico|css|js|svg)$ {
        root /usr/share/nginx/html;
        expires 30d;
        add_header Cache-Control "public, max-age=2592000";
    }

    # Error handling
    error_page 500 502 503 504 /50x.html;
    location = /50x.html {
        root /usr/share/nginx/html;
    }
}


## backend/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libgl1-mesa-glx \
    libglib2.0-0 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code
COPY . .

# Create necessary directories
RUN mkdir -p data/uploads data/results

# Expose port
EXPOSE 8000

# Start FastAPI server
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]


## backend/Dockerfile.worker
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libgl1-mesa-glx \
    libglib2.0-0 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy source code
COPY . .

# Create necessary directories
RUN mkdir -p data/uploads data/results

# Command to run worker is defined in docker-compose.yml


## backend/requirements.txt
fastapi>=0.95.0
uvicorn>=0.22.0
python-multipart>=0.0.6
celery>=5.2.7
redis>=4.5.4
pydantic>=1.10.7
numpy>=1.23.5
opencv-python>=4.7.0
pillow>=9.5.0
torch>=2.0.0
torchvision>=0.15.1
onnx>=1.13.1
onnxruntime>=1.14.1
librosa>=0.10.0
scikit-learn>=1.2.2
python-jose>=3.3.0
passlib>=1.7.4
httpx>=0.24.0
psutil>=5.9.0
aiofiles>=23.1.0
facenet-pytorch>=2.5.2
face-alignment>=1.3.5


## kubernetes/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lightmultidetect-frontend
  labels:
    app: lightmultidetect
    tier: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: lightmultidetect
      tier: frontend
  template:
    metadata:
      labels:
        app: lightmultidetect
        tier: frontend
    spec:
      containers:
      - name: frontend
        image: lightmultidetect/frontend:latest
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "200m"
            memory: "256Mi"
        env:
        - name: REACT_APP_API_URL
          value: /api
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lightmultidetect-backend
  labels:
    app: lightmultidetect
    tier: backend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: lightmultidetect
      tier: backend
  template:
    metadata:
      labels:
        app: lightmultidetect
        tier: backend
    spec:
      containers:
      - name: backend
        image: lightmultidetect/backend:latest
        ports:
        - containerPort: 8000
        resources:
          limits:
            cpu: "1000m"
            memory: "1Gi"
          requests:
            cpu: "500m"
            memory: "512Mi"
        env:
        - name: REDIS_HOST
          value: lightmultidetect-redis
        - name: REDIS_PORT
          value: "6379"
        - name: CELERY_BROKER_URL
          value: redis://lightmultidetect-redis:6379/0
        - name: CELERY_RESULT_BACKEND
          value: redis://lightmultidetect-redis:6379/0
        volumeMounts:
        - name: data-volume
          mountPath: /app/data
        - name: model-volume
          mountPath: /app/ml/models/weights
        livenessProbe:
          httpGet:
            path: /api/health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: lightmultidetect-data-pvc
      - name: model-volume
        persistentVolumeClaim:
          claimName: lightmultidetect-model-pvc
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lightmultidetect-worker
  labels:
    app: lightmultidetect
    tier: worker
spec:
  replicas: 2
  selector:
    matchLabels:
      app: lightmultidetect
      tier: worker
  template:
    metadata:
      labels:
        app: lightmultidetect
        tier: worker
    spec:
      containers:
      - name: worker
        image: lightmultidetect/worker:latest
        command: ["celery", "-A", "backend.tasks.detection_tasks", "worker", "--loglevel=info"]
        resources:
          limits:
            cpu: "2000m"
            memory: "4Gi"
          requests:
            cpu: "1000m"
            memory: "2Gi"
        env:
        - name: REDIS_HOST
          value: lightmultidetect-redis
        - name: REDIS_PORT
          value: "6379"
        - name: CELERY_BROKER_URL
          value: redis://lightmultidetect-redis:6379/0
        - name: CELERY_RESULT_BACKEND
          value: redis://lightmultidetect-redis:6379/0
        - name: C_FORCE_ROOT
          value: "true"
        volumeMounts:
        - name: data-volume
          mountPath: /app/data
        - name: model-volume
          mountPath: /app/ml/models/weights
      volumes:
      - name: data-volume
        persistentVolumeClaim:
          claimName: lightmultidetect-data-pvc
      - name: model-volume
        persistentVolumeClaim:
          claimName: lightmultidetect-model-pvc
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lightmultidetect-redis
  labels:
    app: lightmultidetect
    tier: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: lightmultidetect
      tier: redis
  template:
    metadata:
      labels:
        app: lightmultidetect
        tier: redis
    spec:
      containers:
      - name: redis
        image: redis:alpine
        ports:
        - containerPort: 6379
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "200m"
            memory: "256Mi"
        volumeMounts:
        - name: redis-data
          mountPath: /data
      volumes:
      - name: redis-data
        persistentVolumeClaim:
          claimName: lightmultidetect-redis-pvc


## kubernetes/services.yaml
apiVersion: v1
kind: Service
metadata:
  name: lightmultidetect-frontend
  labels:
    app: lightmultidetect
    tier: frontend
spec:
  selector:
    app: lightmultidetect
    tier: frontend
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: lightmultidetect-backend
  labels:
    app: lightmultidetect
    tier: backend
spec:
  selector:
    app: lightmultidetect
    tier: backend
  ports:
  - port: 8000
    targetPort: 8000
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: lightmultidetect-redis
  labels:
    app: lightmultidetect
    tier: redis
spec:
  selector:
    app: lightmultidetect
    tier: redis
  ports:
  - port: 6379
    targetPort: 6379
  type: ClusterIP


## kubernetes/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: lightmultidetect-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$1
spec:
  rules:
  - host: lightmultidetect.example.com
    http:
      paths:
      - path: /api/(.*)
        pathType: Prefix
        backend:
          service:
            name: lightmultidetect-backend
            port:
              number: 8000
      - path: /(.*)
        pathType: Prefix
        backend:
          service:
            name: lightmultidetect-frontend
            port:
              number: 80


## kubernetes/persistent-volumes.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: lightmultidetect-data-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: lightmultidetect-model-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: lightmultidetect-redis-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi


## kubernetes/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: lightmultidetect-config
data:
  CLEANUP_ON_SHUTDOWN: "true"
  DEFAULT_MODEL: "lightweight_multimodal"


## kubernetes/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: lightmultidetect-secrets
type: Opaque
data:
  SECRET_KEY: YOUR_BASE64_ENCODED_SECRET_KEY  # Replace with actual base64-encoded secret key
  # e.g., use: echo -n "your-secret-key" | base64


## kubernetes/horizontal-pod-autoscaler.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: lightmultidetect-backend-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: lightmultidetect-backend
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: lightmultidetect-worker-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: lightmultidetect-worker
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 80


## kubernetes/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: lightmultidetect-network-policy
spec:
  podSelector:
    matchLabels:
      app: lightmultidetect
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: lightmultidetect
    ports:
    - protocol: TCP
      port: 8000
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: lightmultidetect
    ports:
    - protocol: TCP
      port: 8000
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 6379


## .env.sample
# Backend settings
SECRET_KEY=your-secret-key-here
ACCESS_TOKEN_EXPIRE_MINUTES=11520  # 8 days
CLEANUP_ON_SHUTDOWN=true

# Redis settings
REDIS_HOST=redis
REDIS_PORT=6379

# Celery settings
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

# Frontend settings
REACT_APP_API_URL=http://localhost:8000/api


## deploy.sh
#!/bin/bash

# Deployment script for LightMultiDetect

# Check for required tools
command -v docker >/dev/null 2>&1 || { echo >&2 "Docker is required but not installed. Aborting."; exit 1; }
command -v docker-compose >/dev/null 2>&1 || { echo >&2 "Docker Compose is required but not installed. Aborting."; exit 1; }

# Set environment variables
export COMPOSE_PROJECT_NAME=lightmultidetect

# Stop any running containers
echo "Stopping any running containers..."
docker-compose down

# Build and start containers
echo "Building and starting containers..."
docker-compose up -d --build

# Check container status
echo "Checking container status..."
docker-compose ps

echo "Deployment completed successfully!"
echo "Frontend available at: http://localhost:3000"
echo "Backend API available at: http://localhost:8000/api"


## deploy_k8s.sh
#!/bin/bash

# Kubernetes deployment script for LightMultiDetect

# Check for required tools
command -v kubectl >/dev/null 2>&1 || { echo >&2 "kubectl is required but not installed. Aborting."; exit 1; }

# Set namespace
NAMESPACE="lightmultidetect"

# Create namespace if it doesn't exist
kubectl get namespace $NAMESPACE > /dev/null 2>&1 || kubectl create namespace $NAMESPACE

# Apply configurations
echo "Applying Kubernetes configurations..."

# Apply configs
kubectl apply -f kubernetes/config.yaml -n $NAMESPACE
kubectl apply -f kubernetes/secrets.yaml -n $NAMESPACE

# Apply persistent volumes
kubectl apply -f kubernetes/persistent-volumes.yaml -n $NAMESPACE

# Apply deployments
kubectl apply -f kubernetes/deployments.yaml -n $NAMESPACE

# Apply services
kubectl apply -f kubernetes/services.yaml -n $NAMESPACE

# Apply ingress
kubectl apply -f kubernetes/ingress.yaml -n $NAMESPACE

# Apply network policies
kubectl apply -f kubernetes/network-policy.yaml -n $NAMESPACE

# Apply autoscaling
kubectl apply -f kubernetes/horizontal-pod-autoscaler.yaml -n $NAMESPACE

# Check deployment status
echo "Checking deployment status..."
kubectl get pods -n $NAMESPACE

echo "Kubernetes deployment completed!"
echo "The application will be available at: https://lightmultidetect.example.com (once DNS is configured)"